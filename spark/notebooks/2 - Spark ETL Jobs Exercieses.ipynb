{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be60341-fe75-47c1-b33c-d7bacb1fe4ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Learning Objectives\n",
    "\n",
    "In this notebook, you will craft sophisticated ETL jobs that interface with a variety of common data sources, such as \n",
    "- REST APIs (HTTP endpoints)\n",
    "- RDBMS\n",
    "- Hive tables (managed tables)\n",
    "- Various file formats (csv, json, parquet, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d9fe8dc-6b2e-4499-8961-7e01309d05f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Interview Questions\n",
    "\n",
    "As you progress through the practice, attempt to answer the following questions:\n",
    "\n",
    "## Columnar File\n",
    "- What is a columnar file format and what advantages does it offer?\n",
    "- Why is Parquet frequently used with Spark and how does it function?\n",
    "- How do you read/write data from/to a Parquet file using a DataFrame?\n",
    "\n",
    "## Partitions\n",
    "- How do you save data to a file system by partitions? (Hint: Provide the code)\n",
    "- How and why can partitions reduce query execution time? (Hint: Give an example)\n",
    "\n",
    "## JDBC and RDBMS\n",
    "- How do you load data from an RDBMS into Spark? (Hint: Discuss the steps and JDBC)\n",
    "\n",
    "## REST API and HTTP Requests\n",
    "- How can Spark be used to fetch data from a REST API? (Hint: Discuss making API requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c7f0dcb-2214-41ae-a6f4-12d5a34506ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job One: Parquet file\n",
    "### Extract\n",
    "Extract data from the managed tables (e.g. `bookings_csv`, `members_csv`, and `facilities_csv`)\n",
    "\n",
    "### Transform\n",
    "Data transformation requirements https://pgexercises.com/questions/aggregates/fachoursbymonth.html\n",
    "\n",
    "### Load\n",
    "Load data into a parquet file\n",
    "\n",
    "### What is Parquet? \n",
    "\n",
    "Columnar files are an important technique for optimizing Spark queries. Additionally, they are often tested in interviews.\n",
    "- https://www.youtube.com/watch?v=KLFadWdomyI\n",
    "- https://www.databricks.com/glossary/what-is-parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33324d02-bc67-4c31-b822-8fe8c69fead5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "df = spark.sql(\"SELECT facid, SUM(slots) AS Total_Slots FROM bookings6 WHERE starttime >= '2012-09-01' AND starttime < '2012-10-01' GROUP BY facid ORDER BY SUM(slots);\")\n",
    "df.write.mode(\"overwrite\").parquet(\"/dbfs/FileStore/tables/bookings.parquet\")\n",
    "display(df)\n",
    "display(dbutils.fs.ls(\"/dbfs/FileStore/tables/bookings.parquet\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b51d425e-d532-47e5-8cbf-a91ca78246b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job Two: Partitions\n",
    "\n",
    "### Extract\n",
    "Extract data from the managed tables (e.g. `bookings_csv`, `members_csv`, and `facilities_csv`)\n",
    "\n",
    "### Transform\n",
    "Transform the data https://pgexercises.com/questions/joins/threejoin.html\n",
    "\n",
    "### Load\n",
    "Partition the result data by facility column and then save to `threejoin_delta` managed table. Additionally, they are often tested in interviews.\n",
    "\n",
    "hint: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.partitionBy.html\n",
    "\n",
    "What are partitions? \n",
    "\n",
    "Partitions are an important technique to optimize Spark queries\n",
    "- https://www.youtube.com/watch?v=hvF7tY2-L3U&t=268s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32aea2ca-5178-4034-91ee-c09942c5f518",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "df = spark.sql(\"SELECT DISTINCT mems.firstname || ' ' || mems.surname AS member, facs.name AS facility FROM members7 mems INNER JOIN bookings7 bks ON mems.memid = bks.memid INNER JOIN facilities7 facs ON bks.facid = facs.facid WHERE facs.name in ('Tennis Court 2','Tennis Court 1') ORDER BY member, facility;\")\n",
    "df.write.partitionBy(\"facility\").parquet(\"dbfs:/FileStore/tables/threejoin_delta.parquet\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7610de14-acd6-4374-945d-661dbc08a08e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job Three: HTTP Requests\n",
    "\n",
    "### Extract\n",
    "Extract daily stock price data price from the following companies, Google, Apple, Microsoft, and Tesla. \n",
    "\n",
    "Data Source\n",
    "- API: https://rapidapi.com/alphavantage/api/alpha-vantage\n",
    "- Endpoint: GET `TIME_SERIES_DAILY`\n",
    "\n",
    "Sample HTTP request\n",
    "\n",
    "```\n",
    "curl --request GET \\\n",
    "\t--url 'https://alpha-vantage.p.rapidapi.com/query?function=TIME_SERIES_DAILY&symbol=TSLA&outputsize=compact&datatype=json' \\\n",
    "\t--header 'X-RapidAPI-Host: alpha-vantage.p.rapidapi.com' \\\n",
    "\t--header 'X-RapidAPI-Key: [YOUR_KEY]'\n",
    "\n",
    "```\n",
    "\n",
    "Sample Python HTTP request\n",
    "\n",
    "```\n",
    "import requests\n",
    "\n",
    "url = \"https://alpha-vantage.p.rapidapi.com/query\"\n",
    "\n",
    "querystring = {\n",
    "    \"function\":\"TIME_SERIES_DAILY\",\n",
    "    \"symbol\":\"IBM\",\n",
    "    \"datatype\":\"json\",\n",
    "    \"outputsize\":\"compact\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "    \"X-RapidAPI-Key\": \"[YOUR_KEY]\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "# Now 'data' contains the daily time series data for \"IBM\"\n",
    "```\n",
    "\n",
    "### Transform\n",
    "Find **weekly** max closing price for each company.\n",
    "\n",
    "hints: \n",
    "  - Use a `for-loop` to get stock data for each company\n",
    "  - Use the spark `union` operation to concat all data into one DF\n",
    "  - create a new `week` column from the data column\n",
    "  - use `group by` to calcualte max closing price\n",
    "\n",
    "### Load\n",
    "- Partition `DF` by company\n",
    "- Load the DF in to a managed table called, `max_closing_price_weekly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b76fcc5-fc12-4401-a16c-e24c4c890dd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Company: string (nullable = false)\n |-- Weekly_Max: string (nullable = true)\n |-- Week_Num: integer (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Company</th><th>Weekly_Max</th><th>Week_Num</th></tr></thead><tbody><tr><td>GOOG</td><td>153.7900</td><td>4</td></tr><tr><td>GOOG</td><td>154.8400</td><td>5</td></tr><tr><td>GOOG</td><td>150.2200</td><td>6</td></tr><tr><td>GOOG</td><td>148.7300</td><td>7</td></tr><tr><td>GOOG</td><td>145.3200</td><td>8</td></tr><tr><td>GOOG</td><td>140.1000</td><td>9</td></tr><tr><td>GOOG</td><td>136.2900</td><td>10</td></tr><tr><td>GOOG</td><td>144.3400</td><td>11</td></tr><tr><td>GOOG</td><td>151.7700</td><td>12</td></tr><tr><td>GOOG</td><td>152.2600</td><td>13</td></tr><tr><td>GOOG</td><td>156.5000</td><td>14</td></tr><tr><td>GOOG</td><td>160.7900</td><td>15</td></tr><tr><td>GOOG</td><td>157.4600</td><td>16</td></tr><tr><td>GOOG</td><td>173.6900</td><td>17</td></tr><tr><td>GOOG</td><td>168.9900</td><td>18</td></tr><tr><td>GOOG</td><td>172.9800</td><td>19</td></tr><tr><td>GOOG</td><td>177.2900</td><td>20</td></tr><tr><td>GOOG</td><td>179.5400</td><td>21</td></tr><tr><td>GOOG</td><td>178.0200</td><td>22</td></tr><tr><td>GOOG</td><td>178.3500</td><td>23</td></tr><tr><td>GOOG</td><td>179.5600</td><td>24</td></tr><tr><td>AAPL</td><td>195.1800</td><td>4</td></tr><tr><td>AAPL</td><td>191.7300</td><td>5</td></tr><tr><td>AAPL</td><td>189.4100</td><td>6</td></tr><tr><td>AAPL</td><td>187.1500</td><td>7</td></tr><tr><td>AAPL</td><td>184.3700</td><td>8</td></tr><tr><td>AAPL</td><td>182.6300</td><td>9</td></tr><tr><td>AAPL</td><td>175.1000</td><td>10</td></tr><tr><td>AAPL</td><td>173.2300</td><td>11</td></tr><tr><td>AAPL</td><td>178.6700</td><td>12</td></tr><tr><td>AAPL</td><td>173.3100</td><td>13</td></tr><tr><td>AAPL</td><td>170.0300</td><td>14</td></tr><tr><td>AAPL</td><td>176.5500</td><td>15</td></tr><tr><td>AAPL</td><td>172.6900</td><td>16</td></tr><tr><td>AAPL</td><td>169.8900</td><td>17</td></tr><tr><td>AAPL</td><td>183.3800</td><td>18</td></tr><tr><td>AAPL</td><td>184.5700</td><td>19</td></tr><tr><td>AAPL</td><td>189.8700</td><td>20</td></tr><tr><td>AAPL</td><td>192.3500</td><td>21</td></tr><tr><td>AAPL</td><td>192.2500</td><td>22</td></tr><tr><td>AAPL</td><td>196.8900</td><td>23</td></tr><tr><td>AAPL</td><td>214.2400</td><td>24</td></tr><tr><td>MSFT</td><td>404.8700</td><td>4</td></tr><tr><td>MSFT</td><td>411.2200</td><td>5</td></tr><tr><td>MSFT</td><td>420.5500</td><td>6</td></tr><tr><td>MSFT</td><td>415.2600</td><td>7</td></tr><tr><td>MSFT</td><td>411.6500</td><td>8</td></tr><tr><td>MSFT</td><td>415.5000</td><td>9</td></tr><tr><td>MSFT</td><td>414.9200</td><td>10</td></tr><tr><td>MSFT</td><td>425.2200</td><td>11</td></tr><tr><td>MSFT</td><td>429.3700</td><td>12</td></tr><tr><td>MSFT</td><td>422.8600</td><td>13</td></tr><tr><td>MSFT</td><td>425.5200</td><td>14</td></tr><tr><td>MSFT</td><td>427.9300</td><td>15</td></tr><tr><td>MSFT</td><td>414.5800</td><td>16</td></tr><tr><td>MSFT</td><td>409.0600</td><td>17</td></tr><tr><td>MSFT</td><td>406.6600</td><td>18</td></tr><tr><td>MSFT</td><td>414.7400</td><td>19</td></tr><tr><td>MSFT</td><td>423.0800</td><td>20</td></tr><tr><td>MSFT</td><td>430.5200</td><td>21</td></tr><tr><td>MSFT</td><td>430.3200</td><td>22</td></tr><tr><td>MSFT</td><td>424.5200</td><td>23</td></tr><tr><td>MSFT</td><td>441.5800</td><td>24</td></tr><tr><td>TSLA</td><td>209.1400</td><td>4</td></tr><tr><td>TSLA</td><td>191.5900</td><td>5</td></tr><tr><td>TSLA</td><td>193.5700</td><td>6</td></tr><tr><td>TSLA</td><td>200.4500</td><td>7</td></tr><tr><td>TSLA</td><td>197.4100</td><td>8</td></tr><tr><td>TSLA</td><td>202.6400</td><td>9</td></tr><tr><td>TSLA</td><td>188.1400</td><td>10</td></tr><tr><td>TSLA</td><td>177.7700</td><td>11</td></tr><tr><td>TSLA</td><td>175.6600</td><td>12</td></tr><tr><td>TSLA</td><td>179.8300</td><td>13</td></tr><tr><td>TSLA</td><td>175.2200</td><td>14</td></tr><tr><td>TSLA</td><td>176.8800</td><td>15</td></tr><tr><td>TSLA</td><td>161.4800</td><td>16</td></tr><tr><td>TSLA</td><td>170.1800</td><td>17</td></tr><tr><td>TSLA</td><td>194.0500</td><td>18</td></tr><tr><td>TSLA</td><td>184.7600</td><td>19</td></tr><tr><td>TSLA</td><td>177.5500</td><td>20</td></tr><tr><td>TSLA</td><td>186.6000</td><td>21</td></tr><tr><td>TSLA</td><td>178.7900</td><td>22</td></tr><tr><td>TSLA</td><td>177.9400</td><td>23</td></tr><tr><td>TSLA</td><td>182.4700</td><td>24</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "GOOG",
         "153.7900",
         4
        ],
        [
         "GOOG",
         "154.8400",
         5
        ],
        [
         "GOOG",
         "150.2200",
         6
        ],
        [
         "GOOG",
         "148.7300",
         7
        ],
        [
         "GOOG",
         "145.3200",
         8
        ],
        [
         "GOOG",
         "140.1000",
         9
        ],
        [
         "GOOG",
         "136.2900",
         10
        ],
        [
         "GOOG",
         "144.3400",
         11
        ],
        [
         "GOOG",
         "151.7700",
         12
        ],
        [
         "GOOG",
         "152.2600",
         13
        ],
        [
         "GOOG",
         "156.5000",
         14
        ],
        [
         "GOOG",
         "160.7900",
         15
        ],
        [
         "GOOG",
         "157.4600",
         16
        ],
        [
         "GOOG",
         "173.6900",
         17
        ],
        [
         "GOOG",
         "168.9900",
         18
        ],
        [
         "GOOG",
         "172.9800",
         19
        ],
        [
         "GOOG",
         "177.2900",
         20
        ],
        [
         "GOOG",
         "179.5400",
         21
        ],
        [
         "GOOG",
         "178.0200",
         22
        ],
        [
         "GOOG",
         "178.3500",
         23
        ],
        [
         "GOOG",
         "179.5600",
         24
        ],
        [
         "AAPL",
         "195.1800",
         4
        ],
        [
         "AAPL",
         "191.7300",
         5
        ],
        [
         "AAPL",
         "189.4100",
         6
        ],
        [
         "AAPL",
         "187.1500",
         7
        ],
        [
         "AAPL",
         "184.3700",
         8
        ],
        [
         "AAPL",
         "182.6300",
         9
        ],
        [
         "AAPL",
         "175.1000",
         10
        ],
        [
         "AAPL",
         "173.2300",
         11
        ],
        [
         "AAPL",
         "178.6700",
         12
        ],
        [
         "AAPL",
         "173.3100",
         13
        ],
        [
         "AAPL",
         "170.0300",
         14
        ],
        [
         "AAPL",
         "176.5500",
         15
        ],
        [
         "AAPL",
         "172.6900",
         16
        ],
        [
         "AAPL",
         "169.8900",
         17
        ],
        [
         "AAPL",
         "183.3800",
         18
        ],
        [
         "AAPL",
         "184.5700",
         19
        ],
        [
         "AAPL",
         "189.8700",
         20
        ],
        [
         "AAPL",
         "192.3500",
         21
        ],
        [
         "AAPL",
         "192.2500",
         22
        ],
        [
         "AAPL",
         "196.8900",
         23
        ],
        [
         "AAPL",
         "214.2400",
         24
        ],
        [
         "MSFT",
         "404.8700",
         4
        ],
        [
         "MSFT",
         "411.2200",
         5
        ],
        [
         "MSFT",
         "420.5500",
         6
        ],
        [
         "MSFT",
         "415.2600",
         7
        ],
        [
         "MSFT",
         "411.6500",
         8
        ],
        [
         "MSFT",
         "415.5000",
         9
        ],
        [
         "MSFT",
         "414.9200",
         10
        ],
        [
         "MSFT",
         "425.2200",
         11
        ],
        [
         "MSFT",
         "429.3700",
         12
        ],
        [
         "MSFT",
         "422.8600",
         13
        ],
        [
         "MSFT",
         "425.5200",
         14
        ],
        [
         "MSFT",
         "427.9300",
         15
        ],
        [
         "MSFT",
         "414.5800",
         16
        ],
        [
         "MSFT",
         "409.0600",
         17
        ],
        [
         "MSFT",
         "406.6600",
         18
        ],
        [
         "MSFT",
         "414.7400",
         19
        ],
        [
         "MSFT",
         "423.0800",
         20
        ],
        [
         "MSFT",
         "430.5200",
         21
        ],
        [
         "MSFT",
         "430.3200",
         22
        ],
        [
         "MSFT",
         "424.5200",
         23
        ],
        [
         "MSFT",
         "441.5800",
         24
        ],
        [
         "TSLA",
         "209.1400",
         4
        ],
        [
         "TSLA",
         "191.5900",
         5
        ],
        [
         "TSLA",
         "193.5700",
         6
        ],
        [
         "TSLA",
         "200.4500",
         7
        ],
        [
         "TSLA",
         "197.4100",
         8
        ],
        [
         "TSLA",
         "202.6400",
         9
        ],
        [
         "TSLA",
         "188.1400",
         10
        ],
        [
         "TSLA",
         "177.7700",
         11
        ],
        [
         "TSLA",
         "175.6600",
         12
        ],
        [
         "TSLA",
         "179.8300",
         13
        ],
        [
         "TSLA",
         "175.2200",
         14
        ],
        [
         "TSLA",
         "176.8800",
         15
        ],
        [
         "TSLA",
         "161.4800",
         16
        ],
        [
         "TSLA",
         "170.1800",
         17
        ],
        [
         "TSLA",
         "194.0500",
         18
        ],
        [
         "TSLA",
         "184.7600",
         19
        ],
        [
         "TSLA",
         "177.5500",
         20
        ],
        [
         "TSLA",
         "186.6000",
         21
        ],
        [
         "TSLA",
         "178.7900",
         22
        ],
        [
         "TSLA",
         "177.9400",
         23
        ],
        [
         "TSLA",
         "182.4700",
         24
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Company",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Weekly_Max",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Week_Num",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write your solution here\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import copy\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import weekofyear, udf, monotonically_increasing_id\n",
    "import json\n",
    "compSymbols = [\"GOOG\", \"AAPL\", \"MSFT\", \"TSLA\"]\n",
    "url = \"https://alpha-vantage.p.rapidapi.com/query\"\n",
    "for x in range(4):\n",
    "    querystring = {\n",
    "        \"function\":\"TIME_SERIES_DAILY\",\n",
    "        \"symbol\":compSymbols[x],\n",
    "        \"datatype\":\"json\",\n",
    "        \"outputsize\":\"compact\"\n",
    "    }\n",
    "    headers = {\n",
    "        \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "        \"X-RapidAPI-Key\": \"APIKEY\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, params=querystring)\n",
    "    data = response.json()\n",
    "    # Extract only the data side\n",
    "    df = pd.DataFrame(data['Time Series (Daily)'])\n",
    "    # Get a list of the dates\n",
    "    dates = df.columns\n",
    "    # Transpose the dataframe so that each row is a single date\n",
    "    df_t = df.transpose()\n",
    "    # Convert pandas dataframe to spark dataframe\n",
    "    ddf = spark.createDataFrame(df_t)\n",
    "    # Append the date as a column using the list from before\n",
    "    ddf = ddf.repartition(1).withColumn(\"Date\", udf(lambda id: dates[id])(monotonically_increasing_id()))\n",
    "    # Add a column for the week number using the new date column\n",
    "    ddf = ddf.withColumn('week num', weekofyear('Date'))\n",
    "    #ddf = ddf.withColumn('Company', lit(compSymbols[x]))\n",
    "    if x == 0:\n",
    "        ddf = ddf.withColumn('Company', lit(\"GOOG\"))\n",
    "        unionFrame0 = spark.sql(\"SELECT Company, MAX(`4. close`) AS `Weekly_Max`, `week num` AS Week_Num FROM {table} GROUP BY Company, Week_Num\", table = ddf)\n",
    "    if x == 1:\n",
    "        ddf = ddf.withColumn('Company', lit(\"AAPL\"))\n",
    "        unionFrame1 = spark.sql(\"SELECT Company, MAX(`4. close`) AS `Weekly_Max`, `week num` AS Week_Num FROM {table} GROUP BY Company, Week_Num\", table = ddf)\n",
    "    if x == 2:\n",
    "        ddf = ddf.withColumn('Company', lit(\"MSFT\"))\n",
    "        unionFrame2 = spark.sql(\"SELECT Company, MAX(`4. close`) AS `Weekly_Max`, `week num` AS Week_Num FROM {table} GROUP BY Company, Week_Num\", table = ddf)\n",
    "    if x == 3:\n",
    "        ddf = ddf.withColumn('Company', lit(\"TSLA\"))\n",
    "        unionFrame3 = spark.sql(\"SELECT Company, MAX(`4. close`) AS `Weekly_Max`, `week num` AS Week_Num FROM {table} GROUP BY Company, Week_Num\", table = ddf)\n",
    "        combinedFrame0 = unionFrame0.union(unionFrame1)\n",
    "        combinedFrame1 = combinedFrame0.union(unionFrame2)\n",
    "        combinedFrame2 = combinedFrame1.union(unionFrame3)\n",
    "        dddf = combinedFrame2\n",
    "\n",
    "dddf.printSchema()\n",
    "dddf.write.partitionBy(\"Company\").saveAsTable(\"max_closing_price_weekly\")\n",
    "display(dddf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37f98592-1f5f-4b42-9350-6720e69a7c22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job Four: RDBMS\n",
    "\n",
    "\n",
    "### Extract\n",
    "Extract RNA data from a public PostgreSQL database.\n",
    "\n",
    "- https://rnacentral.org/help/public-database\n",
    "- Extract 100 RNA records from the `rna` table (hint: use `limit` in your sql)\n",
    "- hint: use `spark.read.jdbc` https://docs.databricks.com/external-data/jdbc.html\n",
    "\n",
    "### Transform\n",
    "We want to load the data as it so there is no transformation required.\n",
    "\n",
    "\n",
    "### Load\n",
    "Load the DF in to a managed table called, `rna_100_records`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3011d775-d108-4cb0-85d1-bf21ae1c23d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "remote_table = (spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"url\", \"jdbc:postgresql://hh-pgsql-public.ebi.ac.uk:5432/pfmegrnargs\")\n",
    "  .option(\"user\", \"reader\")\n",
    "  .option(\"password\", \"NWDMCE5xdipIjRrp\")\n",
    "  .option(\"dbtable\", \"rna\")\n",
    "  .option(\"fetchSize\", \"100\")\n",
    "  .load()\n",
    ")\n",
    "# display(remote_table)\n",
    "df = remote_table.select(\"*\").limit(100)\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"rna_100_records\")\n",
    "display(df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2 - Spark ETL Jobs Exercieses",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
